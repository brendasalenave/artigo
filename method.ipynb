{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "\n",
    "### Sentiment Analysis Using NaÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "import unicodedata\n",
    "\n",
    "#importing classes\n",
    "from codes.proc import Proc as P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1\n",
    "***\n",
    "- ##### Data colect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"filename = 'ODemonioeaSrtaPrym.pdf'\\np = P(filename)\\n\\np.convertPDF()\\n\\nname1 = 'Prym'\\nname2 = 'Berta'\\n\\n#Variable 'relation' is used to build training dataset\\n#relation = 'afinidade'\\ntxt = p.get_text()\\np.fname(txt,name1,name2)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'ODemonioeaSrtaPrym.pdf'\n",
    "p = P(filename)\n",
    "\n",
    "p.convertPDF()\n",
    "\n",
    "name1 = 'Prym'\n",
    "name2 = 'estrangeiro'\n",
    "\n",
    "#Variable 'relation' is used to build training dataset\n",
    "#relation = 'afinidade'\n",
    "txt = p.get_text()\n",
    "p.fname(txt,name1,name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- ##### Preprocess\n",
    "\n",
    "In this process we check if the `dataset of training` already exists, if not gets the `training base` and applies preprocess steps and generating `preprocessed-tb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_elements(l1,l2):\n",
    "    l = []\n",
    "    for x in l1:\n",
    "        if x in l2:\n",
    "            l.append(x)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Base\n",
    "tb = 'tb1.csv'\n",
    "# Baseline dataset\n",
    "base = 'base11.csv'\n",
    "\n",
    "if not os.path.exists('./preprocessed-tb.csv'):\n",
    "    sw = []\n",
    "    with open('stopwords.txt') as f:\n",
    "        sw = f.read().splitlines()\n",
    "    with open('preprocessed-tb.csv', 'w', newline='') as outputfile:\n",
    "        with open(tb, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            writer = csv.writer(outputfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "            for row in reader:\n",
    "                #remove accents\n",
    "                data = row[1]\n",
    "                normal = unicodedata.normalize('NFKD', data).encode('ASCII', 'ignore')\n",
    "                row[1] = (str(normal).replace('b\\'','').replace('\\'',''))\n",
    "\n",
    "                #remove hifen\n",
    "                row[1] = row[1].replace('-','')\n",
    "\n",
    "                #lower case\n",
    "                lrow = row[1].split()\n",
    "                lrow = [x.lower() for x in lrow]\n",
    "                \n",
    "                #remove stop words\n",
    "                common = common_elements(sw,lrow)\n",
    "                if len(common) > 0:\n",
    "                    for x in common:\n",
    "                        if x in lrow:\n",
    "                            lrow.remove(x)\n",
    "\n",
    "                row[1] = ' '.join(str(x) for x in lrow)\n",
    "                writer.writerow(row)\n",
    "else:\n",
    "    tb = 'preprocessed-tb.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [] # [(classe, xapareceu)]\n",
    "classes = []\n",
    "tab = []\n",
    "prob = []\n",
    "P1 = []\n",
    "P2 = []\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tb, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in spamreader:\n",
    "        n = n+1\n",
    "        tab.append(row)\n",
    "        #print('tab: ',tab)\n",
    "        if row[0] not in classes:\n",
    "            classes.append(row[0])\n",
    "            c.append((row[0],1))\n",
    "        else:\n",
    "            for (i, j) in enumerate(c):\n",
    "                if(j[0] == row[0]):\n",
    "                    c[i] = ((row[0],c[i][1] + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
