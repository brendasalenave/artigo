{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "\n",
    "### Sentiment Analysis Using NaÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "import unicodedata\n",
    "\n",
    "#importing classes\n",
    "from codes.proc import Proc as P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1\n",
    "***\n",
    "- ##### Data colect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"filename = 'ODemonioeaSrtaPrym.pdf'\\np = P(filename)\\n\\np.convertPDF()\\n\\nname1 = 'Prym'\\nname2 = 'Berta'\\n\\n#Variable 'relation' is used to build training dataset\\n#relation = 'afinidade'\\ntxt = p.get_text()\\np.fname(txt,name1,name2)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'ODemonioeaSrtaPrym.pdf'\n",
    "p = P(filename)\n",
    "\n",
    "p.convertPDF()\n",
    "\n",
    "name1 = 'Prym'\n",
    "name2 = 'estrangeiro'\n",
    "\n",
    "#Variable 'relation' is used to build training dataset\n",
    "#relation = 'afinidade'\n",
    "txt = p.get_text()\n",
    "p.fname(txt,name1,name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- ##### Preprocess\n",
    "\n",
    "In this process we check if the `dataset of training` already exists, if not gets the `training base` and applies preprocess steps and generating `preprocessed-tb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "with open('stopwords.txt') as f:\n",
    "    sw = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for stopwords\n",
    "def common_elements(l1,l2):\n",
    "    l = []\n",
    "    for x in l1:\n",
    "        if x in l2:\n",
    "            l.append(x)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"remove accents\"\n",
    "    data = text\n",
    "    normal = unicodedata.normalize('NFKD', data).encode('ASCII', 'ignore')\n",
    "    text = (str(normal).replace('b\\'','').replace('\\'',''))\n",
    "\n",
    "    \"remove punctuation\"\n",
    "    text = ''.join(c for c in text if c not in punctuation)\n",
    "    #row[1] = row[1].replace('-','')\n",
    "\n",
    "    \"lower case\"\n",
    "    lrow = text.split()\n",
    "    lrow = [x.lower() for x in lrow]\n",
    "    #lrow.pop(0) # remove primeiro elemento (nome)\n",
    "    #lrow.pop() # remove ultimo elemento (nome)\n",
    "    common = common_elements(sw,lrow)\n",
    "    if len(common) > 0:\n",
    "        for x in common:\n",
    "            if x in lrow:\n",
    "                lrow.remove(x)\n",
    "\n",
    "    return ' '.join(str(x) for x in lrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Base\n",
    "tb = 'tb1.csv'\n",
    "# Baseline dataset\n",
    "base = 'base11.csv'\n",
    "\n",
    "if not os.path.exists('./preprocessed-tb.csv'):\n",
    "    with open('preprocessed-tb.csv', 'w', newline='') as outputfile:\n",
    "        with open(tb, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            writer = csv.writer(outputfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "            for row in reader:\n",
    "                row[1] = preprocess(row[1])\n",
    "                writer.writerow(row)\n",
    "else:\n",
    "    tb = 'preprocessed-tb.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [] # [(classe, xapareceu)]\n",
    "classes = []\n",
    "tab = []\n",
    "prob = []\n",
    "P1 = []\n",
    "P2 = []\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tb, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in spamreader:\n",
    "        n = n+1\n",
    "        tab.append(row)\n",
    "        #print('tab: ',tab)\n",
    "        if row[0] not in classes:\n",
    "            classes.append(row[0])\n",
    "            c.append((row[0],1))\n",
    "        else:\n",
    "            for (i, j) in enumerate(c):\n",
    "                if(j[0] == row[0]):\n",
    "                    c[i] = ((row[0],c[i][1] + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Phase 2: Method\n",
    "\n",
    "- ##### Prior Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print('Prior Probability')\n",
    "if '' in classes:\n",
    "    classes.remove('')\n",
    "\n",
    "for x in classes:\n",
    "    x = re.sub(\"[^a-zA-Z|^\\s|^\\t|^\\r]+\", \"\", x)\n",
    "    for i in c:\n",
    "        if i[0] == x:\n",
    "            pi = i[1]/n\n",
    "            break\n",
    "        else:\n",
    "            pi = 0\n",
    "    P1.append((x,pi))\n",
    "    \n",
    "print(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### Constructing the Unigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### Posterior Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(half_left_list,half_right_list):\n",
    "    if i[1] > j[1]:\n",
    "        count_x = count_x + 1\n",
    "    elif i[1] < j[1]:\n",
    "        count_y = count_y + 1\n",
    "    else:\n",
    "        neutral = neutral + 1\n",
    "\n",
    "\n",
    "print('Classification:', nv_class)\n",
    "print(x+':', count_x, y+':', count_y)\n",
    "print(f,nf)\n",
    "print('f->',((f*100)/(nf+f)), 'nf->',(nf*100)/(nf+f))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
