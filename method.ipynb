{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "\n",
    "### Sentiment Analysis Using Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "import unicodedata\n",
    "\n",
    "#importing classes\n",
    "from codes.proc import Proc as P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1\n",
    "***\n",
    "- ##### Data colect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txtbooks/ODemonioeaSrtaPrym.txt\n",
      "estrangeiro para si mesmo, lembrandose das palavras da senhorita Prym\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "\n",
      "Prym podia se espalhar, afugentando caçadores e turistas, ele convocara uma reunião\n",
      "de emergência. Naquele momento, enquanto Chantal se dirigia para a floresta, o estrangeiro\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "\n",
      "estrangeiro havia providenciado tudo, assinado os papéis de transferência\n",
      "do metal, providenciado que o mesmo fosse vendido e o dinheiro depositado na conta recémaberta da senhorita Prym\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'ODemonioeaSrtaPrym.pdf'\n",
    "p = P(filename)\n",
    "\n",
    "p.convertPDF()\n",
    "\n",
    "name1 = 'Prym'\n",
    "name2 = 'estrangeiro'\n",
    "\n",
    "#Variable 'relation' is used to build training dataset\n",
    "#relation = 'afinidade'\n",
    "txt = p.get_text()\n",
    "p.fname(txt,name1,name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- ##### Preprocess\n",
    "\n",
    "In this process we check if the `dataset of training` already exists, if not gets the `training base` and applies preprocess steps and generating `preprocessed-tb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = []\n",
    "with open('stopwords.txt') as f:\n",
    "    sw = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for stopwords\n",
    "def common_elements(l1,l2):\n",
    "    l = []\n",
    "    for x in l1:\n",
    "        if x in l2:\n",
    "            l.append(x)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"remove accents\"\n",
    "    data = text\n",
    "    normal = unicodedata.normalize('NFKD', data).encode('ASCII', 'ignore')\n",
    "    text = (str(normal).replace('b\\'','').replace('\\'',''))\n",
    "\n",
    "    \"remove punctuation\"\n",
    "    text = ''.join(c for c in text if c not in punctuation)\n",
    "    #row[1] = row[1].replace('-','')\n",
    "\n",
    "    \"lower case\"\n",
    "    lrow = text.split()\n",
    "    lrow = [x.lower() for x in lrow]\n",
    "    #lrow.pop(0) # remove primeiro elemento (nome)\n",
    "    \n",
    "    #remove stopwords\n",
    "    common = common_elements(sw,lrow)\n",
    "    if len(common) > 0:\n",
    "        for x in common:\n",
    "            if x in lrow:\n",
    "                lrow.remove(x)\n",
    "\n",
    "    line = ' '.join(str(x) for x in lrow)\n",
    "\n",
    "    # remove digits\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    line = line.translate(remove_digits)\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Base\n",
    "tb = 'tb1.csv'\n",
    "# Baseline dataset\n",
    "base = 'base11.csv'\n",
    "\n",
    "if not os.path.exists('./preprocessed-tb.csv'):\n",
    "    with open('preprocessed-tb.csv', 'w', newline='') as outputfile:\n",
    "        with open(tb, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            writer = csv.writer(outputfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "            for row in reader:\n",
    "                row[1] = preprocess(row[1])\n",
    "                writer.writerow(row)\n",
    "else:\n",
    "    tb = 'preprocessed-tb.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [] # [(class, occurences)]\n",
    "classes = [] # classes names\n",
    "tab = []\n",
    "prob = []\n",
    "P1 = []\n",
    "P2 = []\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tb, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in spamreader:\n",
    "        n = n+1\n",
    "        tab.append(row)\n",
    "        if row[0] not in classes:\n",
    "            classes.append(row[0])\n",
    "            c.append((row[0],1))\n",
    "        else:\n",
    "            for (i, j) in enumerate(c):\n",
    "                if(j[0] == row[0]):\n",
    "                    c[i] = ((row[0],c[i][1] + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Phase 2: Method\n",
    "\n",
    "- ##### Prior Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print('Prior Probability')\n",
    "if '' in classes:\n",
    "    classes.remove('')\n",
    "\n",
    "for x in classes:\n",
    "    x = re.sub(\"[^a-zA-Z|^\\s|^\\t|^\\r]+\", \"\", x)\n",
    "    for i in c:\n",
    "        if i[0] == x:\n",
    "            pi = i[1]/n\n",
    "            break\n",
    "        else:\n",
    "            pi = 0\n",
    "    P1.append((x,pi))\n",
    "    \n",
    "print(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### Constructing the Unigram Language Model & Posterior Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = []\n",
    "count = 0\n",
    "with open(base, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for e in spamreader:\n",
    "        count = count +1\n",
    "        entry = preprocess(e[0])\n",
    "        at = list(set(entry.split()))\n",
    "        for c in classes:\n",
    "            for a in at:\n",
    "                num = 0\n",
    "                for var in tab:\n",
    "                    if c == var[0] and a in var[1]:\n",
    "                        num = num + var[1].count(a)\n",
    "\n",
    "                num_at = 0\n",
    "                for a2 in at:\n",
    "                    for var in tab:\n",
    "                        if c == var[0] and a2 in var[1]:\n",
    "                            num_at = num_at + var[1].count(a2)\n",
    "\n",
    "                    num_at = num_at  + 1\n",
    "\n",
    "                pi = (num +1)/ num_at\n",
    "                prob.append((c,a,pi))\n",
    "\n",
    "        for c in classes:\n",
    "            pi = 1\n",
    "            for e in entry.split():\n",
    "                for p in prob:\n",
    "                    if c == p[0] and e == p[1]:\n",
    "                        pi = (pi*10) * (p[2]*10)                       \n",
    "\n",
    "            P2.append((c,pi))\n",
    "\n",
    "\n",
    "    print('Posterior Probability\\n')\n",
    "    \n",
    "    #Bayes' Rule\n",
    "\n",
    "    for p1 in P1:\n",
    "        for p2 in P2:\n",
    "            if p1[0] is p2[0]:\n",
    "                p = p1[1] * p2[1]\n",
    "                                \n",
    "                bayes.append((p1[0],p))\n",
    "\n",
    "    print('\\n---\\n')\n",
    "    f = 0\n",
    "    nf = 0\n",
    "\n",
    "    for b in bayes:\n",
    "        if b[0] == 'afinidade':\n",
    "            f = f+b[1]\n",
    "        else:\n",
    "            nf = nf+b[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- ##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dbc0c8abd1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnv_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'afinidade'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnv_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'inimizade'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nf' is not defined"
     ]
    }
   ],
   "source": [
    "if f > nf:\n",
    "    nv_class = 'afinidade'\n",
    "else:\n",
    "    nv_class = 'inimizade'\n",
    "\n",
    "half_list = int((len(bayes)/2))\n",
    "half_left_list = bayes[half_list:]\n",
    "half_right_list = bayes[:half_list]\n",
    "\n",
    "#print(hal,half_right_list)\n",
    "x = half_left_list[0][0]\n",
    "y = half_right_list[0][0]\n",
    "count_x = 0\n",
    "count_y = 0\n",
    "neutral = 0\n",
    "\n",
    "for i,j in zip(half_left_list,half_right_list):\n",
    "    if i[1] > j[1]:\n",
    "        count_x = count_x + 1\n",
    "    elif i[1] < j[1]:\n",
    "        count_y = count_y + 1\n",
    "    else:\n",
    "        neutral = neutral + 1\n",
    "\n",
    "\n",
    "print('Classification:', nv_class)\n",
    "print(x+':', count_x, y+':', count_y)\n",
    "print(f,nf)\n",
    "print('f->',((f*100)/(nf+f)), 'nf->',(nf*100)/(nf+f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
